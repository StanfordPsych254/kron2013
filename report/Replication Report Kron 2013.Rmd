---
title: "Replication of Study 1 by Kron et al. (2013, Psychological Science)"
author: "Maia ten Brink (mtb@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

##Introduction

This study replicated Study 1 of "How Are You Feeling? Revisiting the Quantification of Emotional Qualia" by A Kron et al. (2013, *Psychological Science*). This study compared two methods of rating emotional qualia in order to determine whether unipolar pleasant/unpleasant affect rating scales could capture as much or more information than the traditionally used bipolar affect rating scales. The study compared ratings of 72 affective pictures from the International Affective Picture System (IAPS) using either bipolar or unipolar scales. The bipolar scales asked participants to rate their feelings of affective arousal from "calm" to "aroused", and their feelings of affective valence from "very unpleasant" to "very pleasant" with neutral in the center. The unipolar scales asked participants to separately rate their feelings from "no unpleasant feelings"to "strong unpleasant feelings" and from "no pleasant feelings" to "strong pleasant feelings". 

The authors hypothesized that if the dissociation of valence and arousal that has been historically reported in the affect literature indeed reflects distinct dimensions of affective experience, then arousal should be more than the summed intensity of pleasant and unpleasant valence. However, if the sum of pleasant and unpleasant valence does index self-reported arousal, this might indicate that arousal and valence may not in fact represent distinct affective qualia and instead represent differences in how people introspect about or report on affective experiences.

The authors found that summing the unipolar pleasant and unpleasant ratings (`pleasant + unpleasant`) for each picture explained much of the variance ($R^2$ = .57, p < 0.0001) in self-reported arousal. Bipolar valence was not significantly linearly correlated with arousal, and showed a small but significant quadratic relationship ($R^2$ = .37, p = 0.002) with arousal. The difference between unipolar pleasant and unpleasant ratings (`pleasant - unpleasant`) were extremely strongly correlated with bipolar valence (*r* = 0.96, p < 0.0001).

##Methods

###Power Analysis

Kron et al.'s original sample was 30 undergraduates, with an effect size of $\rho$ = 0.7549834.
We used `G*power` to calculate post-hoc power for a two-tailed bivariate correlation of *r* = .75: 

<img src="/writeup/posthocpower.png" /> **Post-hoc power analysis** ![Post-hoc power analysis](/writeup/posthocpower.png)

Post-hoc power was calculated to be 0.99.

For our power analysis, we calculated using `G*power` that we would need a sample size of 11 to achieve 80% power, a sample size of 13 to achieve 90% power, and a sample size of 16 to achieve 95% power.

<img src="/writeup/aprioripower.png" /> **A priori power analysis** ![A priori power analysis](/writeup/aprioripower.png)

###Planned Sample

In our replication study, participants were 90 Amazon Mechanical Turk workers over age 18 located in the United States. Instead of presenting all 72 pictures to all participants, as in the original laboratory study, we varied the IAPS stimuli between subjects in order to reduce the time burden, so that each participant saw one of three blocks of 24 trials. We therefore tripled the original sample size (n = 90).

###Materials

Seventy-two IAPS stimuli were chosen based on a program provided by A. Kron in personal communication. The program's algorithm sampled equally from valence and arousal space by constructing "an 8 x 8 stimulus selection grid. Of the 64 possible cells, 40 cells had at least one entry. If there was more than one picture in a cell, then the one in the left most corner was selected. In addition... each combination of extreme values of valence and arousal were included, resulting in 8 combinations of low and high positive valence, negative valence and arousal. Four pictures were selected from each of these eight combinations, yielding a total of 72 pictures in the stimulus set" (Supplemental Materials, Kron et al., 2013).

We added four practice trials at the start of the experiment (IAPS pictures 5010, 4005, 1820, and 2320), with two bipolar ratings followed by two unipolar ratings.

Instructions on the rating scales were followed exactly from Kron et al. (2013): "Instructions for rating valence and arousal followed the IAPS protocol (Lang et al., 1999). Specifically, for the bipolar valence scale, which ranged from unpleasant feelings (−4) to pleasant feelings (4), participants were told, “At one extreme of the scale, you feel completely unpleasant, unhappy, annoyed, unsatisfied, melancholic, or despaired. At the other end of the scale, you feel completely pleased, happy, satisfied, content, or hopeful.” Participants were further informed that a score of 0, at the scale’s midpoint, indicated “a completely neutral, neither pleasant/happy nor unpleasant/sad state.”
For the arousal scale, which ranged from calm (0) to aroused (8), participants were instructed that “one extreme represents feeling stimulated, excited, frenzied, jittery, wide-awake, or aroused; and at the other end of the scale, you feel completely relaxed, calm, sluggish, dull, sleepy, or bored.” In the separate unipolar scales condition, participants followed the same instructions they had used for the bipolar valence scale except that they were instructed to rate their feelings using two separate scales, one ranging from no pleasant feelings (0) to strong pleasant feelings (8), and the other ranging from no unpleasant feelings (0) to strong unpleasant feelings (8)." Because the format of the ratings was not specified, we opted for radio buttons from 0-8 presented horizontally (see figures below below).

<img src="/writeup/bipolarvalence.png" /> **Bipolar Valence rating scale** ![Bipolar Valence rating scale](/writeup/bipolarvalence.png)

<img src="/writeup/bipolararousal.png" /> **Bipolar Arousal rating scale** ![Bipolar Arousal rating scale](/writeup/bipolararousal.png)

<img src="/writeup/unipolarpleasant.png" /> **Unipolar Valence Pleasant rating scale** ![Unipolar Valence Pleasant rating scale](/writeup/unipolarpleasant.png)

<img src="/writeup/unipolarunpleasant.png" /> **Unipolar Valence Unpleasant rating scale** ![Unipolar Valence Unpleasant rating scale](/writeup/unipolarunpleasant.png)

The experiment consisted of 72 IAPS stimuli presented for 6 seconds each. The stimuli were split between subjects into three 24-trial blocks such that each participant had a 1/3 chance of viewing any block. Four sub-blocks of 6 trials each were presented, with 3 seconds of blank screen between each trial. 

Forty-eight participants completed the study viewing a total of 72 photographs, with 1/3 (16 participants) viewing each block of 24 pictures, and 1/2 (8 participants) rating the 24 pictures with bipolar scales and 1/2 (8 participants) rating the 24 pictures with unipolar scales. 1/2 of the 24 pictures were presented with rating scales in one order ("AB"), and 1/2 of the 24 pictures were presented in the opposite order ("BA").

We designed each sub-block so that it did not include pictures from the closest values of valence and arousal according to the IAPS normative ratings. We ensured that at least one negative and one positive valence images were included in each sub-block. 
We randomized the order of the six trials within each sub-block for each participant. Trials were not randomized between sub-blocks. The order of presentation of the sub-blocks was randomized between participants from four possible permutations (1234, 4321, 3142, or 2413) so that each sub-block could appear in any of the four positions. 
As in the original study, each participant was randomly assigned to a rating order (for example, arousal always rated before bipolar valence, and unpleasant valence always rated before pleasant valence) that remained the same throughout all trials, including practice trials. 
Each sub-block of trials was rated with only one type of scale, either bipolar or unipolar. Each participant was also randomly assigned to a scale order (for example, the first subblock rated with unipolar scales, the second with bipolar scales); this ensured that, between participants, all IAPS stimuli would be rated with both bipolar and unipolar scales. Scale type switched with each sub-block so that there were only ever 6 consecutive trials of one scale type.

###Procedure	

**The experiment can be found here: [Kron Replication](https://web.stanford.edu/~mtb/kron-affect-rating.html)**

At the start of the study, each participant practiced four trials in which they rated two stimuli with the two bipolar scales (arousal and bipolar valence) and two stimuli with the two unipolar scales (pleasant and unpleasant valence). 

Participants then completed 24 trials -- 4 sub-blocks of 6 trials each. Two of the four sub-blocks were followed by the bipolar rating scales, and two by the unipolar rating scales. The rating scale switched each sub-block from one type to the other (unipolar to bipolar, or bipolar to unipolar). The order of rating scales (e.g. arousal first, bipolar valence second) remained the same throughout the experiment. Each image was presented for 6 seconds following a 3-second blank screen. Participants could take as long as necessary to make their rating, but could not move forward without making a rating. The entire experiment lasted around 10 minutes.

At the end of the 24 experimental trials, participants answered three brief questions about age, gender, and morningness-eveningness preference. Participants were invited to add any comments about the study.

At the end of study, participants were debriefed with the following message: "Thank you for participating in the study replicating "How Are You Feeling? Revisiting the Quantification of Emotional Qualia"! The purpose of the study is to understand how people report on their feelings of positive and negative emotion and emotional arousal. In this study, we asked participants to rate how pleasant, unpleasant, or emotionally arousing they judged pictures to be. We expect to find that asking about emotional responses to pictures using two unipolar scales of pleasant and unpleasant affect will capture more information about positive and negative emotion and emotional arousal than the scale measuring pleasant and unpleasant emotion along a single scale. If you would like to learn more about this topic we are happy to provide references."

###Analysis Plan

During preprocessing, data were anonymized and tidied.

We excluded participants who used mobile devices, browser heights less than 700 pixels, or browser widths less than 1000 pixels (indicating that they were not viewing the experiment in full screen). 
Participants who made the same rating on all trials were excluded for having not been appropriately attentive. 
Participants who made more a response in less than 300ms were excluded for not having been appropriately attentive.
Participants who failed to answer the attention check question correctly at the end were excluded.

**Randomization checks:**

We ran a chi-squared goodness-of-fit test to ensure that the four sub-blocks were presented in each of four possible positions an equal number of times across subjects.

```{r}
#This won't work until all data collected:
#chisq.test(x=table(d_filtered$subblock_order),p=c(1,1,1,1))
```

**Check against normative ratings:**

We also compared our participants' ratings to IAPS normative ratings to ensure that our sample roughly matched response patterns in larger populations.

**Key analysis of interest:** 

As our primary index of whether this replication was successful, we focused on a key analysis of interest, a mixed linear regression model of self-reported arousal as predicted by the sum of self-reported pleasant and unpleasant unipolar ratings. Kron and colleagues presented the regression coefficient *r* for the linear correlation between arousal and the sum of pleasant and unpleasant scores.

We therefore replicated their analysis with a mixed linear models. We entered (uncentered) summed pleasant + unpleasant scores as fixed effects to predict arousal ratings scores, with subject and item intercepts and slopes entered as random effects. 

The key results figure from the original paper plots the relationship between summed pleasant and unpleasant rating scores on the X axis and arousal scores on the Y axis.

**Additional analysis:**

We may run further exploratory analysis.

###Differences from Original Study

We did not use exactly the same IAPS stimuli as in Kron's original study, but used Kron's algorithm to generate a new set of stimuli that fit their original parameters.

In the original study, Kron and colleagues presented 72 stimuli for 6 seconds each, with 10-to-21-second intervals of blank screen between each stimulus. Trials were presented in "3" (we suspect this was a typographical error and should have been "4" in order to add up to 72) sub-blocks of six trials, with four sub-blocks per block, and one-minute blank screen intervals between each block. Each sub-block of trials was rated with one type of scale, either bipolar or unipolar. Scale type switched with each sub-block so that there were only ever 6 consecutive trials of one scale type.

In the original study, "the order of pictures in each sub-block was designed in such a way as to not include pictures from two successive squares in the 8 x 8 grid (e.g. the closest values of valence and arousal in the sample) and to ensure that at least two of the pictures were from negative and two were from the positive wings of valence. Order of blocks, order of sub-blocks and order of pictures within sub-block was counterbalanced across participants in such a way that each picture was presented at first, second and third block and different locations within a block. At the same way, ratings according the two models (valence-arousal or pleasant/unpleasant) were counterbalanced so that across participants, a specific sequence of 72 pictures was rated the same amount of times by valence arousal and by pleasant/unpleasant... Four pseudorandom presentation orders were counterbalanced between participants" (Supplemental Materials, Kron et al., 2013).

In our replication, we split the three 24-trial blocks between subjects such that each participant had a 1/3 chance of viewing any block. We reduced the length of inter-stimulus intervals (ISIs) to 3 seconds and removed the jitter because we were not replicating the psychophysiological measures that would have necessitated such long ISIs. 
We designed each of the 4 sub-blocks so that it did not include pictures from the closest values of valence and arousal according to the IAPS normative ratings. We ensured that at least one negative and one positive valence images were included in each sub-block, but could not ensure two of each valence given the random selection of IAPS images generated by Kron's algorithm.
We did not counterbalance trials to appear in different positions in each sub-block, as in the original study, because we were doing between-subjects comparisons, but instead randomized the order of the six trials within each sub-block for each participant. Trials were not randomized between sub-blocks. The order of presentation of the sub-blocks was randomized between participants from four possible permutations (1234, 4321, 3142, or 2413) so that each sub-block could appear in any of the four positions. 
As in the original study, 1/2 of participants were randomly assigned to each rating order (for example, arousal always rated before bipolar valence, and unpleasant valence always rated before pleasant valence) that remained the same throughout all trials, including practice trials. 
Half of participants were also randomly assigned to one of two scale orders (for example, the first subblock rated with unipolar scales, the second with bipolar scales); this ensured that, between participants, all IAPS stimuli would be rated with both bipolar and unipolar scales. 

We also added an attention check at the end of the experiment to ensure that participants were indeed rating their own feelings, not the affect portrayed in the picture. We asked, "What were you rating throughout this task?" Participants could respond: "The feelings portrayed in the pictures" [failed attention check]; "The feelings I experienced, elicited by the pictures"; "Other"; "Neither" [failed attention check].

Finally, we recruited participants online from Amazon Mechanical Turk and presented the experiment online through users' internet browsers. Although we asked participants to use full-screen browsers on personal computers (not mobile devices), we could not control viewing conditions to the extent described in the original study, in which "pictures were presented full screen onto a 19-inch monitor situated 0.5 m away from the participant." We also did not measure any psychophysiological measures during the replication, as in the original. We collected data on browser window size and excluded participants who used mobile devices or browser widths of less than 1000 pixels.


<!-- ### Methods Addendum (Post Data Collection) -->

<!-- You can comment this section out prior to final report with data collection. -->

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.

##Results


### Data preparation

Data preparation followed the analysis plan.
	
```{r include=FALSE, echo=FALSE}
##Clear workspace:
#rm(list=ls())

####Load Relevant Libraries and Functions
library(tidyverse)
library(jsonlite)
library(rjson)
library(RJSONIO)
library(readr)
library(lme4)
library(lmerTest)

Mode <- function(x, na.rm = FALSE) {
  if(na.rm){
    x = x[!is.na(x)]
  }
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}
```

```{r include=FALSE, echo=FALSE}
###Data Preparation

####Import data
# set directory
setwd("/Users/Maia/Desktop/Neuro/Stanford/Classes/PSYCH 254/replication_project/kron2013_254gitrepo")
directory = "../data/"
# read the csv file names to be loaded in the csv directory
files <- dir(directory, pattern =".csv") 

# make an empty data frame to append the data participant by participant
raw_data <- data.frame()

#loop through csv files
for (file in files) {
  #read the csv file into jd
  #jd <- fromJSON(paste(paste(directory, file, sep="")), flatten=TRUE)
  jd <- read_csv(paste(paste(directory, file, sep="")))

  for (i in 1:dim(jd)[1]) { #go through each WorkerID
  #output the Answers.data in json format, then read it back into R using rjsonio and rjsonlite
  answers <- jd$Answer.data[i]
  file.create("answers.json")
  fileConn <- file("answers.json")
  writeLines(answers, fileConn)
  close(fileConn)
  #file.show("answers.json")
  answers <- RJSONIO::fromJSON("answers.json")

  #create a data frame with the column names on the left
  id <- data_frame(rating_order = answers$scale_order,
                  rating_type = answers$scale_type,
                  rating = answers$scale_content,
                  is_practice = answers$is_practice,
                  trialnum = answers$trial,
                  subblock = answers$subblock,
                  block = answers$block, 
                  iaps_trial = answers$iaps_trial,
                  
                  response = answers$rating,
                  duration = answers$elapsed_ms
                  )
  
  id$WorkerID <- jd$WorkerId[i]
  id$subID <- i
  id$gender <- answers$gender
  id$age <- answers$age
  if (is.null(answers$attentioncheck) == FALSE) {
    id$attentioncheck <- answers$attentioncheck
  } else {
    id$attentioncheck <- NA
  }
  id$chronotype <- answers$chronotype
  if (is.null(answers$expt_gen) == FALSE) {
    id$comments <- answers$expt_gen
  } else {
    id$comments <- NA
  }
  id$onmobile = answers$checkmobile
  id$browsertype = answers$user_agent
  id$date = answers$date
  id$AcceptTime = jd$AcceptTime[i]
  id$SubmitTime = jd$SubmitTime[i]
  id$subblock_order <- factor(paste(answers$subblock_order[[1]][1][1],answers$subblock_order[[1]][2][1],answers$subblock_order[[1]][3][1],answers$subblock_order[[1]][4][1], sep=""))
  id$browser_height <- Mode(answers$window_height) #finds browser height used throughout most of experiment
  id$browser_width <- Mode(answers$window_width) #finds browser width used throughout most of experiment
  
  # append the dataframe for that json file to the big data frame 
  raw_data <- bind_rows(raw_data, id)
  
  #remove answers.json file
  rm(answers)
  file.remove("answers.json")
  }
}

#Rearrange column ordering:
raw_data = raw_data %>% 
    dplyr::select(subID, WorkerID, gender:chronotype, attentioncheck, date:SubmitTime, onmobile, browsertype, browser_height, browser_width, subblock_order, rating_order:duration, comments)



### Anonymize WorkerIDs

setwd("/Users/Maia/Desktop/Neuro/Stanford/Classes/PSYCH 254/replication_project/kron2013_254gitrepo/analysis/")

anon_IDkey = data.frame(WorkerID = unique(raw_data$WorkerID), subID = c(1:length(unique(raw_data$WorkerID))))
write.csv(anon_IDkey, file="anonymized-IDs-key.csv")
#Fix subIDs
for (w in 1:dim(anon_IDkey)[1]) {
  pos <- which((raw_data$WorkerID==anon_IDkey$WorkerID[w])==TRUE) #find matching workerIDs
  raw_data$subID[pos] <- anon_IDkey$subID[w]  #replace with unique subIDs
}
#remove WorkerIDs from raw_data
raw_data_anon = raw_data %>%
  dplyr::select(-WorkerID)


#### Participant Exclusion

#create dataframe of all subIDs who responded in a relevant time frame
passed_attentioncheck1 = raw_data_anon %>%
  group_by(subID, SubmitTime) %>%
  summarise(min_duration = min(duration, na.rm = FALSE)) %>%
  mutate(duration_check = min_duration > 300) %>%
  filter(duration_check == TRUE)
ID_passed_attention_check1 = unique(passed_attentioncheck1$subID)

#Positions of each response
poss_resp = data.frame(pleasant = c(0:8), unpleasant = c(0:8), bipolarvalence = c(-4:4), arousal = c(0:8), position = c(1:9))
raw_data_anon <- raw_data_anon %>%
  mutate(resp_position = NA)
for (p in 1:dim(raw_data_anon)[1]) {
    raw_data_anon$resp_position[p] = poss_resp$position[which(raw_data_anon$response[p]==poss_resp[,raw_data_anon$rating[p]])]
}

#create dataframe of all subIDs who responded with same button press on every trial
passed_attentioncheck2 = data.frame()
for (q in 1:length(unique(raw_data_anon$subID))) {   #do this for each subject
  q_raw_data_anon <- raw_data_anon %>%
    filter(subID==q)   #only look at observations for one subID at a time = not working here
  if (isTRUE(all.equal(max(q_raw_data_anon$position), min(q_raw_data_anon$position))) == FALSE) {
    passed_attentioncheck2 = bind_rows(q_raw_data_anon,passed_attentioncheck2) #include this subject (don't exclude)
    }
  rm(q_raw_data_anon)
}
ID_passed_attention_check2 = unique(passed_attentioncheck2$subID)
rm(p,q)

#exclude partcipants 
d_filtered = raw_data_anon %>%
  filter(browser_width >= 1000) %>%
  filter(browser_width >= 700) %>%
  filter(onmobile == FALSE) %>%
  filter(is_practice==FALSE) %>%  #exclude practice trials
  filter(attentioncheck == "pass_attncheck" || is.na(attentioncheck) == TRUE) %>% #in pilotB, filter out NAs
  filter(subID %in% ID_passed_attention_check1) %>%
  filter(subID %in% ID_passed_attention_check2)


### Save Pre-Processed Data

setwd("/Users/Maia/Desktop/Neuro/Stanford/Classes/PSYCH 254/replication_project/kron2013_254gitrepo/analysis/")
save(d_filtered, file="preprocessed_data.R")

```

We did a randomization check, as described in the analysis plan, to check that sub-block order was roughly counterbalanced among participants:

```{r}
#This won't work until all data collected:
#chisq.test(x=table(d_filtered$subblock_order),p=c(1,1,1,1))
```

We also compared our participants' ratings to IAPS normative ratings to ensure that our sample roughly matched response patterns in larger populations.

We also compared our participants' ratings to IAPS normative ratings to ensure that our sample roughly matched response patterns in larger populations.

```{r}
IAPSnorms <- read_csv("../analysis/normativeIAPSratings.csv")

d_comparenorms <- d_regather    ##Fix this!
```


### Confirmatory analysis

The analyses were completed as specified in the analysis plan.  

```{r}
#KEY TEST:
load("../analysis/preprocessed_data.R")

d_filtered$iaps_trial = factor(d_filtered$iaps_trial)
d_filtered$response = as.numeric(d_filtered$response)

d_spread = d_filtered %>% 
  spread(rating,response) %>%
  mutate(sum_pl_un = pleasant + unpleasant, diff_pl_un = pleasant - unpleasant) #calculate sum and diff of unipolar ratings
  
d_regather = d_spread%>%
  gather(rating, response, arousal:diff_pl_un)

#factor variables
d_regather$rating = factor(d_regather$rating)


###### TRY:
d_spread_ctr <- d_filtered %>%
  spread(rating,response) %>%
  mutate(sum_pl_un = pleasant + unpleasant, diff_pl_un = pleasant - unpleasant) %>% #calculate sum and diff of unipolar ratings
  transform(bipolarvalence = scale(bipolarvalence, center=TRUE)) %>% #center all scores
  transform(arousal = scale(arousal, center=TRUE)) %>% #center all scores
  transform(pleasant = scale(pleasant, center=TRUE)) %>% #center all scores
  transform(unpleasant = scale(unpleasant, center=TRUE)) %>% #center all scores
  transform(sum_pl_un = scale(sum_pl_un, center=TRUE)) %>% #center all scores
  transform(diff_pl_un = scale(diff_pl_un, center=TRUE)) #center all scores

d_regather_ctr = d_spread_ctr%>%
  gather(rating, response, arousal:diff_pl_un) %>%
  mutate(predictor = NA) %>%
  mutate(rating_type = NA) %>%
  mutate(bipolarvalence_diff_pl_un = NA) %>%
  mutate(arousal_sum_pl_un = NA) %>%
  filter(rating!="unpleasant") %>% #may not need to remove...
  filter(rating!="pleasant")       #may not need to remove...
for (p in 1:dim(d_regather)[1]) {
  if (d_regather$rating[p]=="arousal" || d_regather$rating[p]=="sum_pl_un") {
    d_regather$predictor[p] = "arousal/sum_pl_un"
  }
  if (d_regather$rating[p]=="bipolarvalence" || d_regather$rating[p]=="diff_pl_un") {
    d_regather$predictor[p] = "bipolarvalence/diff_pl_un"
  }
  if (d_regather$rating[p]=="arousal" || d_regather$rating[p]=="bipolarvalence") {
    d_regather$rating_type[p] = "bipolar"
  }
  if (d_regather$rating[p]=="sum_pl_un" || d_regather$rating[p]=="diff_pl_un") {
    d_regather$rating_type[p] = "unipolar"
  }
  if (d_regather$predictor[p]=="bipolarvalence/diff_pl_un") {
    d_regather$bipolarvalence_diff_pl_un[p] = 1
    d_regather$arousal_sum_pl_un[p] = 0
  }
  if (d_regather$predictor[p]=="arousal/sum_pl_un") {
    d_regather$arousal_sum_pl_un[p] = 1
    d_regather$bipolarvalence_diff_pl_un[p] = 0
  }
}

#factor variables
d_regather_ctr$rating = factor(d_regather_ctr$rating)
d_regather_ctr$predictor = factor(d_regather_ctr$predictor)
d_regather_ctr$rating_type = factor(d_regather_ctr$rating_type)
d_regather_ctr$arousal_sum_pl_un = factor(d_regather_ctr$arousal_sum_pl_un)
d_regather_ctr$bipolarvalence_diff_pl_un = factor(d_regather_ctr$bipolarvalence_diff_pl_un)


#Models:
#mixed model with random intercept effects
keyModel1 = lmer(arousal ~ sum_pl_un + (1|subID) + (1|iaps_trial), data=d_spread, REML = TRUE)
summary(keyModel1)

#mixed model with random intercept and slope effects
keyModel2 = lmer(arousal ~ sum_pl_un + (1|subID) + (predictor|subID) + (rating_type|subID) + (1|iaps_trial) + (predictor|iaps_trial) + (rating_type|iaps_trial), data=d_regather, REML = TRUE)
summary(keyModel2)

AIC(keyModel1, keyModel2)
```

We recreated Kron et al.'s original figure:
<img src="/writeup/keyanalysis.png" /> **Key Result** ![Key Result](/writeup/keyanalysis.png)

```{r}
#Plot arousal ~ PL + UN
ggplot(data=d_regather, 
  aes(x = (rating==sum_pl_un), y = (rating==arousal))) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) + 
  xlab("Centered (Pleasant + Unpleasant) Ratings") +
  ylab("Centered Arousal Ratings")
```


###Exploratory analyses

Any follow-up analyses desired (not required). 


```{r include = FALSE}
#Plot bipolar valence ~ PL - UN
ggplot(data=d_spread, 
  aes(x = diff_pl_un, y = bipolarvalence)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) + 
  xlab("Centered (Pleasant - Unpleasant) Ratings") +
  ylab("Centered Bipolar Valence Ratings")
```

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.

### References

Kron, A., Goldstein, A., Lee, D.H.J., Garhouse, K., & Anderson, A.K. (2013). How Are You Feeling? Revisiting the Quantification of Emotional Qualia. *Psychological Science*, *24*:1503-1511.

Lang, P. J., Bradley, M. M., & Cuthbert, B. N. (1999). International Affective Picture System (IAPS): Affective ratings of pictures and instruction manual. Gainesville: University of Florida, Center for the Study of Emotion and Attention.
